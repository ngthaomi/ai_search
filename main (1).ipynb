{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c758cf4",
   "metadata": {},
   "source": [
    "## **AI Search with RefinedWeb Dataset and OLMo 2 Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfc934-f454-4547-822e-c400c561776f",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [0. Setup](#0-setup)  \n",
    "- [1. Data Loading](#1-data-loading)  \n",
    "- [2. Data Exploration](#2-data-exploration)  \n",
    "- [3. Data Preprocessing](#3-data-preprocessing)  \n",
    "    - [3.1 Data Cleaning](#31-data-cleaning)   \n",
    "    - [3.2 Feature Engineering](#32-feature-engineering)  \n",
    "- [4. Brand Sentiment Analysis](#4-brand-sentiment-analysis)  \n",
    "    - [4.1 Lexicon-Based](#41-lexicon-based)  \n",
    "    - [4.2 Transformer-based](#42-transformer-based)\n",
    "- [5. Brand-Specific Analysis](#5-brand-specific-analysis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcf565-b4d6-4ee3-b89d-c84c6eae06ef",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f55d557-6caf-4f8b-866e-4b955cdcc7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy>=2.0.2 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: pyspark>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (3.8.7)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (19.0.0)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2024.11.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (6.29.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.7.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.17.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.33.2)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (0.23.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (2025.3.0)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (2.14.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (4.48.0)\n",
      "Requirement already satisfied: spark-nlp==5.1.3 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (5.1.3)\n",
      "Requirement already satisfied: xformers in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.0.30)\n",
      "Requirement already satisfied: torchao in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (0.11.0)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (5.0.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (1.11.0)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.3.26)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->-r requirements.txt (line 12)) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 20)) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.2.3->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=2.2.3->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.12/site-packages (from pyspark>=3.5.0->-r requirements.txt (line 4)) (0.10.9.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk>=3.8->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk>=3.8->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (2.10.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.12/site-packages (from spacy>=3.0.0->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 7)) (3.2.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 8)) (0.70.16)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.12/site-packages (from fastparquet->-r requirements.txt (line 10)) (2.10.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 11)) (5.14.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 16)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence-transformers->-r requirements.txt (line 25)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers->-r requirements.txt (line 25)) (1.15.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 27)) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 27)) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 27)) (0.4.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain->-r requirements.txt (line 27)) (2.0.25)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (3.11.12)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 11)) (4.3.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain->-r requirements.txt (line 27)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain->-r requirements.txt (line 27)) (1.33)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 6)) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 20)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 20)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 20)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 20)) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 27)) (3.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 6)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 6)) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 6)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 6)) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->-r requirements.txt (line 12)) (3.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 25)) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.18.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain->-r requirements.txt (line 27)) (3.0.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.2.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 6)) (1.17.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 11)) (0.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r requirements.txt (line 27)) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319f76d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13a1817-50a8-4971-ba8b-aa21a5dd4883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, col, udf, pandas_udf, PandasUDFType, arrays_zip, array_contains, substring, length, explode, first, avg, when, monotonically_increasing_id\n",
    "from pyspark.sql.functions import to_date, dayofmonth, month, year\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, FloatType, BooleanType, ArrayType, StructType, StructField\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from huggingface_hub import HfApi\n",
    "# from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from emoji import demojize\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datasets import load_dataset\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cefc4-be05-4082-81ae-80cbd6ec1f6c",
   "metadata": {},
   "source": [
    "# 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95441793-6047-4a4b-acd7-39027884dba6",
   "metadata": {},
   "source": [
    "## 1.1 Generating Paths Files\n",
    "This section of the code generates `refinedweb_paths.txt` with URLs to Parquet files for each dataset. If the files already exist, the code verifies them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4967d710-2cb7-4e52-abd9-74fe81ab0a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/refinedweb/refinedweb_paths.txt already exists. Verifying contents...\n",
      "data/refinedweb/refinedweb_paths.txt is valid with 5534 URLs.\n"
     ]
    }
   ],
   "source": [
    "def generate_paths_file(dataset_id, output_file, directory_prefix=None):\n",
    "    api = HfApi()\n",
    "    \n",
    "    # List all files in the dataset repository\n",
    "    files = api.list_repo_files(repo_id=dataset_id, repo_type=\"dataset\")\n",
    "    \n",
    "    # Filter for Parquet files \n",
    "    parquet_urls = [\n",
    "        f\"https://huggingface.co/datasets/{dataset_id}/resolve/main/{f}\"\n",
    "        for f in files if f.endswith(\".parquet\") and (directory_prefix is None or f.startswith(directory_prefix))\n",
    "    ]\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # If file exists, verify contents\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists. Verifying contents...\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            existing_urls = set(line.strip() for line in f if line.strip())\n",
    "        if set(parquet_urls).issubset(existing_urls):\n",
    "            print(f\"{output_file} is valid with {len(existing_urls)} URLs.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Updating {output_file} with new URLs...\")\n",
    "    # Save URLs to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for url in parquet_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"Saved {len(parquet_urls)} URLs to {output_file}\")\n",
    "    \n",
    "\n",
    "# Generate paths for RefinedWeb\n",
    "generate_paths_file(\"tiiuae/falcon-refinedweb\", \"data/refinedweb/refinedweb_paths.txt\", directory_prefix=\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0a6f7-4dae-4faf-bfd7-f891fc51001f",
   "metadata": {},
   "source": [
    "## 1.2 Download Dataset\n",
    "The following code runs the `download_parquet.sh` script to download Parquet files for both datasets. The files will be saved to `data/refinedweb/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643d348f-1010-46ef-807e-820b4e083583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset_name, paths_file):\n",
    "    paths_file = os.path.abspath(paths_file)\n",
    "    if not os.path.exists(paths_file):\n",
    "        print(f\"Error: {paths_file} does not exist. Skipping download for {dataset_name}.\")\n",
    "        return False\n",
    "    if not os.path.getsize(paths_file) > 0:\n",
    "        print(f\"Error: {paths_file} is empty. Skipping download for {dataset_name}.\")\n",
    "        return False\n",
    "    print(f\"Downloading {dataset_name} dataset...\")\n",
    "    try:\n",
    "        log_file = f\"data/{dataset_name}/download.log\"\n",
    "        os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "        with open(log_file, \"w\") as f:\n",
    "            process = subprocess.Popen(\n",
    "                [\"bash\", \"download_parquet.sh\", dataset_name, paths_file],\n",
    "                stdout=f,\n",
    "                stderr=f,\n",
    "                text=True\n",
    "            )\n",
    "            process.wait(timeout=3600)  # 1 hour timeout\n",
    "        with open(log_file, \"r\") as f:\n",
    "            print(f.read())\n",
    "        return process.returncode == 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error running download script for {dataset_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# # Downloads RefinedWeb parquet files\n",
    "# download_dataset(\"refinedweb\", \"data/refinedweb/refinedweb_paths.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9cbf27-4ab4-4d40-9905-37679a809457",
   "metadata": {},
   "source": [
    "We will consequently verify the downloaded Parquet files to ensure they are accessible and readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f344fc1-f7da-492c-a698-ac0d436c1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parquet files in data/refinedweb: 961\n",
      "Error reading data/refinedweb/train-00051-of-05534-b8a6dc2a89918893.parquet: name 'spark' is not defined\n"
     ]
    }
   ],
   "source": [
    "def verify_parquet_files(directory):\n",
    "    # Find all Parquet files in the directory\n",
    "    parquet_files = glob.glob(f\"{directory}/*.parquet\")\n",
    "    print(f\"Number of Parquet files in {directory}: {len(parquet_files)}\")\n",
    "    \n",
    "    if parquet_files:\n",
    "        # Try reading the first Parquet file as a sample using Spark\n",
    "        try:\n",
    "            # Read the first Parquet file into a Spark DataFrame\n",
    "            df_sample = spark.read.parquet(parquet_files[0])\n",
    "            print(f\"Number of rows per Parquet file: {df_sample.count()}\")\n",
    "            \n",
    "            # Check for null values in all columns\n",
    "            print(\"\\nNull value counts per column:\")\n",
    "            from pyspark.sql.functions import col\n",
    "            for column in df_sample.columns:\n",
    "                null_count = df_sample.filter(col(column).isNull()).count()\n",
    "                print(f\"{column}: {null_count} nulls\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {parquet_files[0]}: {e}\")\n",
    "    else:\n",
    "        print(f\"No Parquet files found in {directory}\")\n",
    "        \n",
    "verify_parquet_files(\"data/refinedweb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb38b5b-e84c-4cc3-ab27-8d66a0eed25e",
   "metadata": {},
   "source": [
    "## 1.3 Load and Filter Documents with Brand Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b099f94-a495-4232-80f7-467eb105be7f",
   "metadata": {},
   "source": [
    "PySpark is utilised to process the large-scale dataset. The Spark Session was initialised below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bee1c8-3212-4355-8628-2fe4a9fb1251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 17:47:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AI Search Pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# For more detailed logging\n",
    "# sc = spark.sparkContext\n",
    "# sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a53af7a-6c6e-457e-957a-ce729303e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "def load_and_filter_data(input_dir, output_file):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # Create filtered table\n",
    "    brands = [\"HSBC\", \"Barclays\", \"Lloyds\", \"NatWest\"]\n",
    "    brand_conditions = ' AND '.join([f\"LOWER(content) LIKE '%{brand.lower()}%'\" for brand in brands])  # All brands must be mentioned\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE filtered_brands AS\n",
    "        SELECT *\n",
    "        FROM '{os.path.join(input_dir, \"*.parquet\")}'\n",
    "        WHERE {brand_conditions}\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    \n",
    "    # Export to Parquet\n",
    "    con.execute(f\"\"\"\n",
    "        COPY filtered_brands TO '{output_file}' (FORMAT PARQUET)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Filtered + Exported to {output_file}\")\n",
    "    \n",
    "    # Close the connection\n",
    "    con.close()\n",
    "\n",
    "#load_and_filter_data(\"data/refinedweb\", \"data/filtered_data/brands_articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f1be1a-a5f4-4c75-a152-87b65daf2e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+---------------+----------------+--------------------+\n",
      "|             content|                 url|          timestamp|           dump|         segment|          image_urls|\n",
      "+--------------------+--------------------+-------------------+---------------+----------------+--------------------+\n",
      "|Thousands of UK e...|https://newsypeop...|2021-02-24 20:52:11|CC-MAIN-2021-10| 1614178347321.0|[[https://i.daily...|\n",
      "|20 November 2015:...|http://your.natio...|2016-04-28 21:45:04|CC-MAIN-2016-18|1461860109830.69|                  []|\n",
      "|Written by TotalA...|https://arsenalar...|2022-09-24 18:58:45|CC-MAIN-2022-40|1664030333455.97|[[https://arsenal...|\n",
      "|The History of PP...|http://www.better...|2014-04-16 13:31:30|CC-MAIN-2014-15|1397609523429.20|                  []|\n",
      "|Die Erkenntnis, d...|http://choshi-kou...|2018-08-14 14:18:58|CC-MAIN-2018-34|1534221209040.29|[[cta-button_jetz...|\n",
      "|Subchapters:\\n- B...|https://www.800zi...|2022-09-24 23:50:30|CC-MAIN-2022-40|1664030333541.98|                  []|\n",
      "|We’re always work...|https://blog.with...|2021-07-23 19:59:32|CC-MAIN-2021-31|1627046150000.59|[[https://blog.wi...|\n",
      "|UK banks paying o...|https://www.teleg...|2021-01-15 22:29:56|CC-MAIN-2021-04| 1610703496947.2|[[/content/dam/bu...|\n",
      "|- The Observer, S...|http://www.guardi...|2009-07-10 08:10:49|      crawl-002|   crawl-002-017|                  []|\n",
      "|Firm Profile > Mc...|https://www.legal...|2020-07-02 16:40:31|CC-MAIN-2020-29| 1593655879532.0|                  []|\n",
      "|. The public fina...|http://www.eca-wa...|2017-08-16 17:19:45|CC-MAIN-2017-34|1502886102309.55|                  []|\n",
      "|bank of cyprus uk...|http://h1.nichost...|2018-10-15 14:10:56|CC-MAIN-2018-43|1539583509196.33|                  []|\n",
      "|\\nYour questions ...|http://ecofinders...|2021-01-15 21:45:43|CC-MAIN-2021-04| 1610703496947.2|                  []|\n",
      "|It’s Finished\\nJo...|https://www.lrb.c...|2017-03-23 02:13:18|CC-MAIN-2017-13| 1490218186608.9|                  []|\n",
      "|Can You Be Sure M...|http://blog.phono...|2021-04-10 19:11:58|CC-MAIN-2021-17| 1618038057476.6|                  []|\n",
      "|When moving to th...|https://katsgoneg...|2019-10-14 01:40:15|CC-MAIN-2019-43| 1570986648481.7|                  []|\n",
      "|UPWARDLY MOBILE\\n...|https://designate...|2019-03-19 00:05:53|CC-MAIN-2019-13| 1552912201812.2|[[https://designa...|\n",
      "|Free Business Int...|http://www.aerode...|2009-07-02 16:36:12|      crawl-002|   crawl-002-017|[[http://www.aero...|\n",
      "|NatWest shares se...|https://www.ig.co...|2021-10-16 00:58:40|CC-MAIN-2021-43|1634323583087.95|                  []|\n",
      "|With the number o...|https://marketbus...|2020-10-19 20:48:32|CC-MAIN-2020-45| 1603107866404.1|                  []|\n",
      "+--------------------+--------------------+-------------------+---------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data/filtered_data/brands_articles.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe512e55-0a75-4062-8a31-d470ccf88562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents mentioning all 4 brands together: 350\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents mentioning all 4 brands together: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf5523-4530-43bf-81c8-2816dcbc3b85",
   "metadata": {},
   "source": [
    "# 2. Data Pre-processing\n",
    "\n",
    "This segment cleans and transforms the raw dataset to make it suitable for sentiment analysis, removing noise and extracting useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5334f35e-1462-4c08-b3e4-4dc6081f69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for non-null/non-empty text\n",
    "df = df.filter(col(\"content\").isNotNull() & (col(\"content\") != \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1db2b07-e952-4a7f-bb7d-fe2fc9cbb6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|             content|                 url|          timestamp|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|Thousands of UK e...|https://newsypeop...|2021-02-24 20:52:11|\n",
      "|20 November 2015:...|http://your.natio...|2016-04-28 21:45:04|\n",
      "|Written by TotalA...|https://arsenalar...|2022-09-24 18:58:45|\n",
      "|The History of PP...|http://www.better...|2014-04-16 13:31:30|\n",
      "|Die Erkenntnis, d...|http://choshi-kou...|2018-08-14 14:18:58|\n",
      "|Subchapters:\\n- B...|https://www.800zi...|2022-09-24 23:50:30|\n",
      "|We’re always work...|https://blog.with...|2021-07-23 19:59:32|\n",
      "|UK banks paying o...|https://www.teleg...|2021-01-15 22:29:56|\n",
      "|- The Observer, S...|http://www.guardi...|2009-07-10 08:10:49|\n",
      "|Firm Profile > Mc...|https://www.legal...|2020-07-02 16:40:31|\n",
      "|. The public fina...|http://www.eca-wa...|2017-08-16 17:19:45|\n",
      "|bank of cyprus uk...|http://h1.nichost...|2018-10-15 14:10:56|\n",
      "|\\nYour questions ...|http://ecofinders...|2021-01-15 21:45:43|\n",
      "|It’s Finished\\nJo...|https://www.lrb.c...|2017-03-23 02:13:18|\n",
      "|Can You Be Sure M...|http://blog.phono...|2021-04-10 19:11:58|\n",
      "|When moving to th...|https://katsgoneg...|2019-10-14 01:40:15|\n",
      "|UPWARDLY MOBILE\\n...|https://designate...|2019-03-19 00:05:53|\n",
      "|Free Business Int...|http://www.aerode...|2009-07-02 16:36:12|\n",
      "|NatWest shares se...|https://www.ig.co...|2021-10-16 00:58:40|\n",
      "|With the number o...|https://marketbus...|2020-10-19 20:48:32|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping irrelevant columns\n",
    "df = df.drop(\"dump\", \"segment\", \"image_urls\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34717098-9445-45d7-9863-c9f69677bc38",
   "metadata": {},
   "source": [
    "# 2.1 Filter Valid URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d978e5b9-2b44-49f5-b98d-da185e652695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import BooleanType\n",
    "import time\n",
    "\n",
    "# Function to check if a URL is valid\n",
    "def is_url_valid(url):\n",
    "    if not url:\n",
    "        return False\n",
    "    try:\n",
    "        # Send a HEAD request to minimize data transfer\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        # Consider 200 as valid; you can adjust to include other codes (e.g., 301, 302)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        # Handle connection errors, timeouts, etc.\n",
    "        return False\n",
    "\n",
    "is_url_valid_udf = udf(is_url_valid, BooleanType())\n",
    "\n",
    "df = df.withColumn(\"is_url_valid\", is_url_valid_udf(col(\"url\"))) \n",
    "# df.select(\"url\", \"is_url_valid\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df431251-66b7-4908-b359-6023c1264f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where URL is valid\n",
    "df = df.filter(col(\"is_url_valid\") == True)\n",
    "\n",
    "# print(f\"Number of documents with valid URLs mentioning all 4 brands: {df.count()}\")\n",
    "# df.select(\"url\", \"is_url_valid\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b884908b-842c-40f3-9957-485b005b8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"is_url_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89fa9f2b-5134-4116-b5c7-aeb096949f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the DataFrame as Parquet\n",
    "# output_path = \"data/filtered_data/valid_articles.parquet\"\n",
    "\n",
    "# df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25f763f5-34af-4083-8575-1262fcdff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/filtered_data/valid_articles.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718665e2-5cc1-4f93-aea4-2862796c27c3",
   "metadata": {},
   "source": [
    "## 2.2 Deduplication by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f1d0b7-1ef6-4156-8456-607c23d28d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# URL date patterns\n",
    "url_date_patterns = [\n",
    "    r'/(\\d{4})/([a-z]{3})/(\\d{2})/',\n",
    "    r'/(\\d{4})/(\\d{2})/(\\d{2})/',\n",
    "    r'/(\\d{4})-(\\d{2})-(\\d{2})/',\n",
    "    r'/(\\d{4})\\.(\\d{2})\\.(\\d{2})/',\n",
    "    r'/(\\d{4})_(\\d{2})_(\\d{2})/',\n",
    "    r'(\\d{4})/(\\d{2})(\\d{2})/',\n",
    "    r'/(\\d{4})/(\\d{2})/',\n",
    "    r'/(\\d{4})-(\\d{2})/',\n",
    "    r'(\\d{4})[-_\\.](\\d{2})[-_\\.](\\d{2})',\n",
    "    r'post[-_]?(\\d{4})[-_](\\d{2})[-_](\\d{2})',\n",
    "    r'(\\d{8})',\n",
    "]\n",
    "\n",
    "month_abbrev_map = {\n",
    "    \"jan\": \"01\", \"feb\": \"02\", \"mar\": \"03\", \"apr\": \"04\",\n",
    "    \"may\": \"05\", \"jun\": \"06\", \"jul\": \"07\", \"aug\": \"08\",\n",
    "    \"sep\": \"09\", \"oct\": \"10\", \"nov\": \"11\", \"dec\": \"12\"\n",
    "}\n",
    "\n",
    "# TEXT patterns\n",
    "text_date_patterns = [\n",
    "    r'(?:Published|Posted|Updated|Created|First published|Last updated)[:\\s]*([A-Za-z]{3,9}[\\s\\-.,]?\\d{1,2}(?:st|nd|rd|th)?[\\s,]+(?:\\d{4}))',\n",
    "    r'(?:Published|Posted|Updated|Date|Created)[:\\s]*([\\d]{1,2}[\\s\\-/.][A-Za-z]{3,9}[\\s\\-/,]+[\\d]{4})',\n",
    "    r'(?:Published|Posted|Updated|Date)[:\\s]*([\\d]{4}[-/\\.][\\d]{1,2}[-/\\.][\\d]{1,2})',\n",
    "    r'([A-Za-z]{3,9}\\s\\d{4})',\n",
    "    r'(\\d{4}/\\d{2}/\\d{2})',\n",
    "    r'(\\d{2}[-/\\.]\\d{2}[-/\\.]\\d{4})',\n",
    "    r'(\\d{4}[-/\\.]\\d{2})',\n",
    "    r'/(\\d{4})/(\\d{1,2})/(\\d{1,2})/',\n",
    "    r'/(\\d{4})/(\\d{1,2})/'\n",
    "]\n",
    "\n",
    "# Combining extraction from 'url', revert to 'content' if date not found in url\n",
    "@F.udf(StringType())\n",
    "def extract_combined_date_udf(url, text):\n",
    "    def try_url_date(url_str):\n",
    "        if not url_str or not isinstance(url_str, str):\n",
    "            return None\n",
    "        if not re.search(r'https?://', url_str):\n",
    "            return None\n",
    "        for pattern in url_date_patterns:\n",
    "            match = re.search(pattern, url_str, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                try:\n",
    "                    parts = match.groups()\n",
    "                    if len(parts) == 3:\n",
    "                        year, month, day = parts\n",
    "                        if month.isalpha():\n",
    "                            month = month_abbrev_map.get(month.lower())\n",
    "                            if not month:\n",
    "                                continue\n",
    "                    elif len(parts) == 2:\n",
    "                        year, month = parts\n",
    "                        day = \"01\"\n",
    "                    elif len(parts) == 1:\n",
    "                        val = parts[0]\n",
    "                        if len(val) == 8:\n",
    "                            year, month, day = val[:4], val[4:6], val[6:]\n",
    "                        elif len(val) == 4:\n",
    "                            year, month, day = val, \"01\", \"01\"\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "                    y, m, d = int(year), int(month), int(day)\n",
    "                    if y < 1900 or y > datetime.now().year + 1:\n",
    "                        continue\n",
    "                    if m < 1 or m > 12:\n",
    "                        continue\n",
    "                    if d < 1 or d > 31:\n",
    "                        continue\n",
    "                    return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "                except:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def try_text_date(text_str):\n",
    "        if not text_str or not isinstance(text_str, str):\n",
    "            return None\n",
    "        text_str = re.sub(r'\\s+', ' ', text_str).strip()\n",
    "        current_year = datetime.now().year\n",
    "        probable_dates = []\n",
    "        for pattern in text_date_patterns:\n",
    "            matches = re.findall(pattern, text_str)\n",
    "            for match in matches:\n",
    "                raw = ' '.join(match) if isinstance(match, tuple) else match\n",
    "                try:\n",
    "                    parsed = parser.parse(raw, fuzzy=True)\n",
    "                    year = parsed.year\n",
    "                    if 1900 <= year <= current_year + 1:\n",
    "                        probable_dates.append(parsed.date().isoformat())\n",
    "                except:\n",
    "                    continue\n",
    "        return min(probable_dates) if probable_dates else None\n",
    "\n",
    "    # Try extracting from URL first\n",
    "    date_from_url = try_url_date(url)\n",
    "    if date_from_url:\n",
    "        return date_from_url\n",
    "\n",
    "    # If failed, try from text\n",
    "    return try_text_date(text)\n",
    "\n",
    "df = df.withColumn(\"published_date\", extract_combined_date_udf(F.col(\"url\"), F.col(\"content\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e98a7b-7c27-4ab5-a8dd-10ddd6ebb691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Written by TotalArsenal\\nYour dreams are china in y...</td>\n",
       "      <td>https://arsenalarsenal.net/2011/05/11/is-it-right-to-push-for-wenger%E2%80%99s-departure/</td>\n",
       "      <td>2022-09-24 18:58:45</td>\n",
       "      <td>2011-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We’re always working on new ways to make your mone...</td>\n",
       "      <td>https://blog.withplum.com/plum-interest-available-for-all/</td>\n",
       "      <td>2021-07-23 19:59:32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- The Observer, Sunday 7 May 2000\\nlarger | smaller...</td>\n",
       "      <td>http://www.guardian.co.uk/money/2000/may/07/personalfinancenews.observercashsection2/print</td>\n",
       "      <td>2009-07-10 08:10:49</td>\n",
       "      <td>2000-05-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. The public finance comes in the form of soft loa...</td>\n",
       "      <td>http://www.eca-watch.org/taxonomy/term/77</td>\n",
       "      <td>2017-08-16 17:19:45</td>\n",
       "      <td>2009-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s Finished\\nJohn Lanchester here’s the really an...</td>\n",
       "      <td>https://www.lrb.co.uk/v31/n10/john-lanchester/its-finished</td>\n",
       "      <td>2017-03-23 02:13:18</td>\n",
       "      <td>1912-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can You Be Sure Methods To\\n100 day loans\\nDetails A...</td>\n",
       "      <td>http://blog.phonographen.com/2013/04/30/a-way-to-reports-in-relation-to/</td>\n",
       "      <td>2021-04-10 19:11:58</td>\n",
       "      <td>2013-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When moving to the UK I had no idea I would be sta...</td>\n",
       "      <td>https://katsgoneglobal.com/credit-score-new-uk-resident/</td>\n",
       "      <td>2019-10-14 01:40:15</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>With the number of bank branch closures reaching 4...</td>\n",
       "      <td>https://marketbusinessnews.com/bank-branch-closures-reach-470-2014-triggering-call-vince-cable/43377/</td>\n",
       "      <td>2020-10-19 20:48:32</td>\n",
       "      <td>1989-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finance\\nCo-op goes ahead with deal to snap up Lloy...</td>\n",
       "      <td>http://www.equities.com/news/headline-story?dt=2012-07-16&amp;val=275856&amp;cat=finance</td>\n",
       "      <td>2013-05-18 20:04:22</td>\n",
       "      <td>2012-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Partner with us\\nWe believe that by working in part...</td>\n",
       "      <td>https://www.moneyadvicetrust.org/partnerships/partner-with-us/</td>\n",
       "      <td>2021-04-10 15:10:55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David Shinkins: Great, thank you very much. So goo...</td>\n",
       "      <td>https://www.barclayscorporate.com/insights/regulations/confirmation-of-payee/</td>\n",
       "      <td>2021-01-15 20:01:29</td>\n",
       "      <td>2019-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None of the current accounts offered by Barclays, ...</td>\n",
       "      <td>http://www.standard.co.uk/news/how-big-banks-failed-the-test-on-value-and-service-6602411.html</td>\n",
       "      <td>2017-03-23 01:16:47</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cash-strapped borrowers who are tempted into takin...</td>\n",
       "      <td>http://www.theguardian.com/money/2012/nov/17/payday-loans-credit-rating</td>\n",
       "      <td>2015-03-26 23:53:53</td>\n",
       "      <td>2012-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JEFF PRESTRIDGE: It may be time to put a Tesco ban...</td>\n",
       "      <td>https://www.thisismoney.co.uk/money/comment/article-2651435/JEFF-PRESTRIDGE-Put-Tesco-bank-account-shopping-list.html</td>\n",
       "      <td>2021-09-16 23:10:32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New protection for individuals tricked into transf...</td>\n",
       "      <td>https://www.ghventuresuk.com/post/scam-victims-to-be-refunded-by-banks</td>\n",
       "      <td>2020-09-18 20:23:06</td>\n",
       "      <td>2016-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Worried about the world banking crisis and how it ...</td>\n",
       "      <td>http://www.mirror.co.uk/news/top-stories/2008/10/03/bank-crisis-your-questions-answered-by-john-husband-115875-20764007/</td>\n",
       "      <td>2009-07-06 10:26:55</td>\n",
       "      <td>2008-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Picture: Claudia Jones\\nSpeech by Trevor Rayne at t...</td>\n",
       "      <td>https://democracyandclasstruggle.blogspot.com/2010/04/british-imperialism-in-world-today-and.html</td>\n",
       "      <td>2022-01-29 01:25:29</td>\n",
       "      <td>2010-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rising Stars\\nAwards\\n2021\\nWelcome to the\\nHow do we ...</td>\n",
       "      <td>http://view.ceros.com/incisive-media/incisive-awards-computing-rising-star-awards-3</td>\n",
       "      <td>2021-09-16 22:12:26</td>\n",
       "      <td>2019-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Press release\\nBusinesses to access millions in gov...</td>\n",
       "      <td>https://www.gov.uk/government/news/businesses-to-access-millions-in-government-export-support-through-partnership-with-high-street-banks</td>\n",
       "      <td>2018-03-17 11:05:06</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>slef-reflections on Online Banking with GNU/Linux,...</td>\n",
       "      <td>http://mjr.towers.org.uk/blog/2007/banking</td>\n",
       "      <td>2018-11-12 22:14:43</td>\n",
       "      <td>2005-07-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      content  \\\n",
       "0      Written by TotalArsenal\\nYour dreams are china in y...   \n",
       "1       We’re always working on new ways to make your mone...   \n",
       "2      - The Observer, Sunday 7 May 2000\\nlarger | smaller...   \n",
       "3       . The public finance comes in the form of soft loa...   \n",
       "4      It’s Finished\\nJohn Lanchester here’s the really an...   \n",
       "5     Can You Be Sure Methods To\\n100 day loans\\nDetails A...   \n",
       "6       When moving to the UK I had no idea I would be sta...   \n",
       "7       With the number of bank branch closures reaching 4...   \n",
       "8      Finance\\nCo-op goes ahead with deal to snap up Lloy...   \n",
       "9      Partner with us\\nWe believe that by working in part...   \n",
       "10      David Shinkins: Great, thank you very much. So goo...   \n",
       "11      None of the current accounts offered by Barclays, ...   \n",
       "12      Cash-strapped borrowers who are tempted into takin...   \n",
       "13      JEFF PRESTRIDGE: It may be time to put a Tesco ban...   \n",
       "14      New protection for individuals tricked into transf...   \n",
       "15      Worried about the world banking crisis and how it ...   \n",
       "16     Picture: Claudia Jones\\nSpeech by Trevor Rayne at t...   \n",
       "17  Rising Stars\\nAwards\\n2021\\nWelcome to the\\nHow do we ...   \n",
       "18     Press release\\nBusinesses to access millions in gov...   \n",
       "19      slef-reflections on Online Banking with GNU/Linux,...   \n",
       "\n",
       "                                                                                                                                         url  \\\n",
       "0                                                  https://arsenalarsenal.net/2011/05/11/is-it-right-to-push-for-wenger%E2%80%99s-departure/   \n",
       "1                                                                                 https://blog.withplum.com/plum-interest-available-for-all/   \n",
       "2                                                 http://www.guardian.co.uk/money/2000/may/07/personalfinancenews.observercashsection2/print   \n",
       "3                                                                                                  http://www.eca-watch.org/taxonomy/term/77   \n",
       "4                                                                                 https://www.lrb.co.uk/v31/n10/john-lanchester/its-finished   \n",
       "5                                                                   http://blog.phonographen.com/2013/04/30/a-way-to-reports-in-relation-to/   \n",
       "6                                                                                   https://katsgoneglobal.com/credit-score-new-uk-resident/   \n",
       "7                                      https://marketbusinessnews.com/bank-branch-closures-reach-470-2014-triggering-call-vince-cable/43377/   \n",
       "8                                                           http://www.equities.com/news/headline-story?dt=2012-07-16&val=275856&cat=finance   \n",
       "9                                                                             https://www.moneyadvicetrust.org/partnerships/partner-with-us/   \n",
       "10                                                             https://www.barclayscorporate.com/insights/regulations/confirmation-of-payee/   \n",
       "11                                            http://www.standard.co.uk/news/how-big-banks-failed-the-test-on-value-and-service-6602411.html   \n",
       "12                                                                   http://www.theguardian.com/money/2012/nov/17/payday-loans-credit-rating   \n",
       "13                     https://www.thisismoney.co.uk/money/comment/article-2651435/JEFF-PRESTRIDGE-Put-Tesco-bank-account-shopping-list.html   \n",
       "14                                                                    https://www.ghventuresuk.com/post/scam-victims-to-be-refunded-by-banks   \n",
       "15                  http://www.mirror.co.uk/news/top-stories/2008/10/03/bank-crisis-your-questions-answered-by-john-husband-115875-20764007/   \n",
       "16                                         https://democracyandclasstruggle.blogspot.com/2010/04/british-imperialism-in-world-today-and.html   \n",
       "17                                                       http://view.ceros.com/incisive-media/incisive-awards-computing-rising-star-awards-3   \n",
       "18  https://www.gov.uk/government/news/businesses-to-access-millions-in-government-export-support-through-partnership-with-high-street-banks   \n",
       "19                                                                                                http://mjr.towers.org.uk/blog/2007/banking   \n",
       "\n",
       "             timestamp published_date  \n",
       "0  2022-09-24 18:58:45     2011-05-11  \n",
       "1  2021-07-23 19:59:32           None  \n",
       "2  2009-07-10 08:10:49     2000-05-07  \n",
       "3  2017-08-16 17:19:45     2009-07-07  \n",
       "4  2017-03-23 02:13:18     1912-07-07  \n",
       "5  2021-04-10 19:11:58     2013-04-30  \n",
       "6  2019-10-14 01:40:15           None  \n",
       "7  2020-10-19 20:48:32     1989-07-07  \n",
       "8  2013-05-18 20:04:22     2012-07-16  \n",
       "9  2021-04-10 15:10:55           None  \n",
       "10 2021-01-15 20:01:29     2019-12-07  \n",
       "11 2017-03-23 01:16:47           None  \n",
       "12 2015-03-26 23:53:53     2012-11-17  \n",
       "13 2021-09-16 23:10:32           None  \n",
       "14 2020-09-18 20:23:06     2016-12-07  \n",
       "15 2009-07-06 10:26:55     2008-10-03  \n",
       "16 2022-01-29 01:25:29     2010-04-01  \n",
       "17 2021-09-16 22:12:26     2019-07-07  \n",
       "18 2018-03-17 11:05:06           None  \n",
       "19 2018-11-12 22:14:43     2005-07-07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark df\n",
    "# df.select(\"content\",\"url\", \"timestamp\", \"published_date\").show(truncate=False)\n",
    "\n",
    "# # Pandas df for readability\n",
    "pdf = df.select(\"content\", \"url\", \"timestamp\", \"published_date\").toPandas()\n",
    "# Truncate content\n",
    "pdf[\"content\"] = pdf[\"content\"].str[:50] + \"...\" \n",
    "\n",
    "# Display full URL\n",
    "with pd.option_context(\"display.max_colwidth\", None):  \n",
    "    display(pdf.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cbcba3b-c7b5-4a53-804a-5266c5b9453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------+\n",
      "|             content|                 url|          timestamp|published_date|\n",
      "+--------------------+--------------------+-------------------+--------------+\n",
      "|We’re always work...|https://blog.with...|2021-07-23 19:59:32|          NULL|\n",
      "|It’s Finished\\nJo...|https://www.lrb.c...|2017-03-23 02:13:18|    1912-07-07|\n",
      "|A academia AQUASP...|http://www.aquasp...|2019-04-18 14:35:47|    1917-07-07|\n",
      "|Water day, the 4t...|http://lasmangist...|2017-10-16 23:55:16|    1922-07-07|\n",
      "|Warwick's Sanctio...|https://blogs.war...|2017-10-17 07:32:46|    1930-07-07|\n",
      "|With midterms, pa...|http://vanderbilt...|2019-04-18 17:17:50|    1935-02-07|\n",
      "|The Royal Bank of...|https://wiki2.org...|2020-11-23 20:13:21|    1946-07-07|\n",
      "|Banking News Lloy...|https://www.fairi...|2021-09-16 21:38:08|    1951-07-07|\n",
      "|Is something we d...|http://lasmangist...|2017-10-17 00:23:57|    1953-07-07|\n",
      "|Banking Crisis Ti...|https://www.credi...|2016-02-06 00:08:24|    1958-07-07|\n",
      "|The Hongkong and ...|https://themktgbo...|2022-08-07 19:18:14|    1968-07-07|\n",
      "|Funding the great...|https://www.racon...|2019-10-14 07:50:56|    1970-07-07|\n",
      "|The Female FTSE B...|http://docplayer....|2018-01-16 11:46:10|    1980-07-07|\n",
      "|RBS INDEPENDENT L...|http://docplayer....|2018-04-19 19:51:13|    1983-04-07|\n",
      "|Auctions in the s...|https://www.asset...|2019-04-18 17:01:31|    1985-07-07|\n",
      "|The PAM 50 Most I...|https://www.pamin...|2021-10-15 22:37:52|    1987-07-07|\n",
      "|With the number o...|https://marketbus...|2020-10-19 20:48:32|    1989-07-07|\n",
      "|We don’t have a t...|http://www.thisis...|2014-10-20 11:18:27|    1990-07-07|\n",
      "|Zyra.NET //// Zyr...|http://www.zyra.g...|2021-11-27 03:32:02|    1996-07-07|\n",
      "|\\nFinancial analy...|http://www.euromo...|2017-08-16 15:16:43|    1997-01-07|\n",
      "|- .\\nThe address ...|http://www.snazzy...|2018-12-10 04:28:22|    1997-07-07|\n",
      "|LLP post formatio...|https://www.codda...|2017-09-19 16:57:52|    1998-07-07|\n",
      "|- The Observer, S...|http://www.guardi...|2009-07-10 08:10:49|    2000-05-07|\n",
      "|'Why I love This ...|https://www.thisi...|2019-12-06 03:45:06|    2000-07-07|\n",
      "|Which?, the Consu...|http://www.liverp...|2017-01-16 13:03:15|    2000-11-07|\n",
      "|- The Guardian, S...|http://www.guardi...|2009-07-14 21:06:21|    2001-02-10|\n",
      "|- The Observer, S...|http://www.guardi...|2009-07-20 01:33:30|    2001-07-15|\n",
      "|What's Happening ...|http://www.lovemo...|2012-05-24 15:45:44|    2001-10-07|\n",
      "|Good news if you'...|http://www.thegua...|2015-04-18 14:04:19|    2002-01-19|\n",
      "| 23rd quarterly U...|http://marketrese...|2017-08-16 15:08:57|    2002-03-07|\n",
      "|Budget and bring ...|https://www.thisi...|2021-11-27 08:27:29|    2002-07-07|\n",
      "|<pre>\\nBANKING\\n1...|https://www.manag...|2020-03-28 15:31:11|    2003-07-07|\n",
      "|Switching to a mo...|http://www.thegua...|2014-04-16 11:06:03|    2004-01-17|\n",
      "|Make sure you rem...|http://israelxmxk...|2018-04-19 15:03:51|    2004-07-07|\n",
      "|slef-reflections ...|http://mjr.towers...|2018-11-12 22:14:43|    2005-07-07|\n",
      "|When.\\nJanuary fi...|http://www.talkta...|2012-05-26 20:30:25|    2006-02-07|\n",
      "|- The Guardian, S...|http://www.guardi...|2009-07-09 23:41:21|    2006-02-25|\n",
      "|Author: Adam Romb...|https://www.gfmag...|2022-09-24 20:19:56|    2006-07-07|\n",
      "|Clydesdale, Lloyd...|http://www.waleso...|2009-07-05 21:52:42|    2006-07-12|\n",
      "|How the big banks...|https://www.daily...|2018-10-15 16:30:39|    2007-02-07|\n",
      "|Treasury’s five-y...|http://www.bbc.co...|2009-07-10 05:00:53|    2007-11-01|\n",
      "|- The Observer, S...|http://www.guardi...|2009-07-13 16:01:42|    2008-06-08|\n",
      "|Banking for the b...|http://www.lalkar...|2019-06-16 03:52:44|    2008-07-07|\n",
      "|Paypal Disputes –...|https://sosfakefl...|2018-12-09 23:24:46|    2008-09-15|\n",
      "|Worried about the...|http://www.mirror...|2009-07-06 10:26:55|    2008-10-03|\n",
      "|Lewis Martin, age...|http://theautomat...|2019-10-14 06:14:31|    2009-01-01|\n",
      "|– in Westminster ...|https://www.theyw...|2022-05-16 13:15:32|    2009-01-27|\n",
      "|BANKING\\nBarclays...|https://www.btgad...|2020-01-17 16:04:14|    2009-03-07|\n",
      "|. The public fina...|http://www.eca-wa...|2017-08-16 17:19:45|    2009-07-07|\n",
      "| that have been c...|http://www.webroo...|2016-04-29 01:59:45|    2010-02-10|\n",
      "+--------------------+--------------------+-------------------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents mentioning all 4 brands together after deduplication: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates([\"published_date\"])\n",
    "    \n",
    "df.show(50)\n",
    "print(f\"Number of documents mentioning all 4 brands together after deduplication: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657a545-425e-467a-8706-cf509cb5c7e9",
   "metadata": {},
   "source": [
    "# 3. Extracting Implicit Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c970b5a-6fa4-4563-8407-afe78b5b08f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.12/site-packages (0.46.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /opt/conda/lib/python3.12/site-packages (from bitsandbytes) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.33.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28e7d36e-8e8a-43ee-9222-365d695a2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "def extract_rankings(df):\n",
    "    # Get the first row\n",
    "    first_row = df.first()\n",
    "    if not first_row:\n",
    "        print(\"No data available in the DataFrame.\")\n",
    "        return {}\n",
    "    \n",
    "    # Initialize tokenizer and model with 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b\",\n",
    "        quantization_config=quantization_config,\n",
    "        device = \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Categorize the content of the first row\n",
    "    content = first_row[\"content\"]\n",
    "    topic_prompt = f\"Categorize the following text into an overarching topic (e.g., Sustainability, Finance): {content}\\nTopic:\"\n",
    "    inputs = tokenizer(topic_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, num_return_sequences=1)\n",
    "    topic = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Topic:\")[-1].strip() if \"Topic:\" in tokenizer.decode(outputs[0], skip_special_tokens=True) else \"Unknown\"\n",
    "    \n",
    "    print(f\"First row content categorized as: {topic}\")\n",
    "    return {\"0\": topic}  # Return as a dictionary with index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9de3e0e-d066-417e-b730-536a14658544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topics \u001b[38;5;241m=\u001b[39m \u001b[43mextract_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, topic \u001b[38;5;129;01min\u001b[39;00m topics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Topic = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m, in \u001b[0;36mextract_rankings\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2-2b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically maps to available devices (e.g., CUDA or CPU)\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Move model to GPU if available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:3620\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3617\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3620\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3627\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3628\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "topics = extract_rankings(df)\n",
    "for index, topic in topics.items():\n",
    "    print(f\"Index {index}: Topic = {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926c4bc-4c73-4fd8-8507-61172b7bc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def categorize_content(iterator):\n",
    "    # Initialize pipeline once per partition\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"google/gemma-2-2b\",\n",
    "        device=device,\n",
    "        torch_dtype=torch.float16,\n",
    "        max_new_tokens=10  # Reduced to minimize memory\n",
    "    )\n",
    "    \n",
    "    # Process each row in the partition\n",
    "    for row in iterator:\n",
    "        content = row[\"content\"]\n",
    "        topic_prompt = f\"Categorize the following text into an overarching topic (e.g., Sustainability, Finance): {content}\\nTopic:\"\n",
    "        outputs = pipe(topic_prompt, num_return_sequences=1)\n",
    "        topic = outputs[0][\"generated_text\"].split(\"Topic:\")[-1].strip() if \"Topic:\" in outputs[0][\"generated_text\"] else \"Unknown\"\n",
    "        yield {\"index\": row[\"index\"], \"topic\": topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9d48b-724d-413f-b839-b61ced2ee581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rankings(df, max_articles=300):\n",
    "    # Limit to max_articles and add index for tracking\n",
    "    df = df.limit(max_articles).withColumn(\"index\", monotonically_increasing_id())\n",
    "    \n",
    "    # Define UDF to categorize content\n",
    "    categorize_udf = udf(lambda x: next(categorize_content([x.asDict()])), MapType(StringType(), StringType()))\n",
    "    \n",
    "    # Apply UDF to create a new column with topics\n",
    "    df_with_topics = df.rdd.mapPartitions(categorize_content).toDF().select(\"index\", \"topic\")\n",
    "    \n",
    "    # Join back to original df to maintain all columns\n",
    "    df_with_topics = df.join(df_with_topics, \"index\", \"left\")\n",
    "    \n",
    "    # Collect topics as a dictionary for further processing\n",
    "    topics = dict(df_with_topics.select(\"index\", \"topic\").collect())\n",
    "    \n",
    "    for index, topic in topics.items():\n",
    "        print(f\"Index {index}: Topic = {topic}\")\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75944774-0581-47d9-bb58-2b16768e2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = extract_rankings(df)\n",
    "for topic, ranks in rankings.items():\n",
    "    print(f\"Topic: {topic}, Average Rankings: {ranks}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42808b41-c1a6-43c6-a262-c348dbc0dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_rankings(df, max_articles=300):\n",
    "    # Convert Spark DataFrame to Pandas for LLM processing (limit to max_articles)\n",
    "    pdf = df.select(\"content\").limit(max_articles).toPandas()\n",
    "    \n",
    "    # Load Gemma 2 model and tokenizer locally\n",
    "    model_name = \"google/gemma-2-2b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "    \n",
    "    # Store rank sums and counts per brand per topic\n",
    "    rank_sums = defaultdict(lambda: defaultdict(float))\n",
    "    rank_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Process each article\n",
    "    for content in pdf[\"content\"]:\n",
    "        # Categorize topic using LLM\n",
    "        topic_prompt = f\"Categorize the following text into a topic (e.g., Sustainability, Finance): {content}\\nTopic:\"\n",
    "        inputs = tokenizer(topic_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, num_return_sequences=1)\n",
    "        topic = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Topic:\")[-1].strip()\n",
    "        \n",
    "        # Extract implicit ranking\n",
    "        ranking_prompt = f\"Summarize the following article and extract the implicit ranking of [HSBC, Barclays, Lloyds, NatWest] based on their mentions in the context of {topic}: {content}\\nRanking:\"\n",
    "        inputs = tokenizer(ranking_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1)\n",
    "        ranking_text = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Ranking:\")[-1].strip()\n",
    "        \n",
    "        # Parse ranking (assign ranks based on order of mention)\n",
    "        brands = [\"HSBC\", \"Barclays\", \"Lloyds\", \"NatWest\"]\n",
    "        ranking = []\n",
    "        for brand in brands:\n",
    "            if brand.lower() in ranking_text.lower():\n",
    "                ranking.append(brand)\n",
    "        if ranking:\n",
    "            for rank, brand in enumerate(ranking, 1):\n",
    "                rank_sums[topic][brand] += rank\n",
    "                rank_counts[topic][brand] += 1\n",
    "    \n",
    "    # Calculate average rankings\n",
    "    average_rankings = {}\n",
    "    for topic in rank_sums:\n",
    "        average_rankings[topic] = {\n",
    "            brand: rank_sums[topic][brand] / rank_counts[topic][brand]\n",
    "            for brand in rank_sums[topic]\n",
    "        }\n",
    "    \n",
    "    return average_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf64ba-e51c-4d1c-b7b6-603a2793480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract implicit rankings\n",
    "rankings = extract_rankings(df)\n",
    "for topic, ranks in rankings.items():\n",
    "    print(f\"Topic: {topic}, Average Rankings: {ranks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d665d93-9e1b-46ff-aa01-642cf53b6865",
   "metadata": {},
   "source": [
    "# 4. Baseline LLM Brand Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e70f5-30c3-46a7-bb7e-5e3c705aa474",
   "metadata": {},
   "source": [
    "# 5. LLM Brand Ranking with RAG Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e9a4e-1803-4dc3-845c-62d2f1b51d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7179e8-8d87-4b7e-8af6-ab51d99b1e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c7891-2b09-45a8-b5e5-d3b5f8791084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a2e94-2300-40d1-83ef-76e9d0ccc07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98db5d-274b-41b1-8576-47a7f14d6404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ff5f-6aff-48f3-abde-b4cb33733a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f83a64ca-fb0e-41d8-90e9-1bc75c293dc3",
   "metadata": {},
   "source": [
    "## 2.1 Loading RefinedWeb Parquet files into a Single Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4b540-eb16-42c2-8ea2-285aeb9624ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refinedweb_dir = \"data/refinedweb\"\n",
    "df = spark.read.parquet(f\"{refinedweb_dir}/*.parquet\")\n",
    "\n",
    "# print(f\"RefinedWeb dataset has {df.count()} rows in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd65ba1-9c84-446b-87f8-dfa031c80edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477d0f2-fcbd-458b-938c-b6dfc199160e",
   "metadata": {},
   "source": [
    "RefinedWeb columns are explained below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b74cac-9a2c-4da7-b86e-be96825ca350",
   "metadata": {},
   "source": [
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `content` | The main textual content of the document record (e.g., the body of a document, article, or code snippet). This will be used as the primary field for training language model and analysis in our study. |\n",
    "| `url` | The URL of the web page or resource from which the content was sourced. |\n",
    "| `timestamp` | The date and time when the web page was crawled or the data was extracted from the source (e.g., Common Crawl). |\n",
    "| `dump` | This refers to the specific Common Crawl (CC) dump from which the data was sourced. CC releases monthly dumps (e.g., CC-MAIN-2023-06), allowing users to trace the data back to its original crawl.|\n",
    "| `segment` | Identifies the segment or subset of the Common Crawl dump from which the record originates.|\n",
    "| `image_urls` | A list of URLs pointing to images found on the web page.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcd526-9a24-47f9-adb0-ca7339659808",
   "metadata": {},
   "source": [
    "## 3.1 Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f31c3e-442f-4902-be10-b486fd817bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenisation - clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = re.sub(r'[\\n\\r]', ' ', text)     # removes newlines and carriage returns\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())     # removes punctuation and lowercase\n",
    "    text = re.sub(r'\\d+', '', text)   # removes digits\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # whitespace\n",
    "    text = demojize(text)  # convert emojis to text (e.g., 😊 → :smiling_face:)\n",
    "    text = re.sub(r'[^\\w\\s:]', '', text.lower())  # preserve emoji tokens\n",
    "    return text\n",
    "\n",
    "clean_udf = udf(clean_text, StringType())\n",
    "df = df.withColumn(\"clean_text\", clean_udf(col(\"content\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eb2af-c74d-452f-83eb-281953a328d8",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering\n",
    "\n",
    "The aim of this part is to extract additional information and columns from the data to enable more detailed sentiment analysis, such as brand mentions and content types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88274bd4-6ec3-4496-9dd6-cf9cae6a6832",
   "metadata": {},
   "source": [
    "### Extraction of Brand Mentions: *brand_mention* and *mention_count*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16fff8-3f9b-486d-9cea-f10cfa011864",
   "metadata": {},
   "source": [
    "Our analysis is focused on the following **4 brands**: HSBC, LLoyds, Barclays, and Revolut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bfa47-8d4f-4962-9499-67de1aa2a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK banks\n",
    "BRANDS = [\"barclays\", \"lloyds\", \"hsbc\", \"monzo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d339509-959a-4f4f-9d60-cb93029b110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking-related context terms to confirm brand relevance\n",
    "BANKING_CONTEXT = [\n",
    "    \"finance\", \"financial\", \"bank\", \"banking\", \"account\", \"savings\", \"current\", \"mortgage\", \"loan\", \"credit\", \"debit\", \"card\",\n",
    "    \"app\", \"mobile\", \"online\", \"branch\", \"atm\", \"transfer\", \"fees\", \"overdraft\", \"service\", \"support\"\n",
    "]\n",
    "\n",
    "# Negative context terms to exclude false positives\n",
    "NEGATIVE_CONTEXT = {\n",
    "    \"revolut\": [\"revolution\", \"revolutionary\", \"national revolution\"],\n",
    "    \"barclays\": [\"barclays center\", \"barclays arena\"],\n",
    "    \"lloyds\": [\"lloyds of london\"], \n",
    "    \"hsbc\": [],    \n",
    "    \"monzo\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886964a7-a241-48b4-b5fb-e706143abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_brands_and_counts(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return [], []\n",
    "#     text_lower = text.lower()\n",
    "#     tokens = word_tokenize(text_lower)\n",
    "    \n",
    "#     brands_found = []\n",
    "#     counts = []\n",
    "    \n",
    "#     for brand in BRANDS:\n",
    "#         # Initialize count\n",
    "#         brand_count = 0\n",
    "        \n",
    "#         # Check for brand in tokens with word boundaries\n",
    "#         brand_pattern = r'\\b' + re.escape(brand) + r'\\b'\n",
    "#         matches = re.findall(brand_pattern, text_lower)\n",
    "#         brand_count += len(matches)\n",
    "        \n",
    "#         # Validate with banking context (at least one banking term nearby)\n",
    "#         has_banking_context = False\n",
    "#         for context in BANKING_CONTEXT:\n",
    "#             if context in text_lower:\n",
    "#                 has_banking_context = True\n",
    "#                 break\n",
    "        \n",
    "#         # Check for negative context to exclude false positives\n",
    "#         has_negative_context = False\n",
    "#         for negative_term in NEGATIVE_CONTEXT.get(brand, []):\n",
    "#             if negative_term in text_lower:\n",
    "#                 has_negative_context = True\n",
    "#                 break\n",
    "        \n",
    "#         # Only include brand if it has banking context and no negative context\n",
    "#         if brand_count > 0 and has_banking_context and not has_negative_context:\n",
    "#             brands_found.append(brand)\n",
    "#             counts.append(brand_count)\n",
    "    \n",
    "#     return brands_found, counts\n",
    "\n",
    "# @udf(ArrayType(StringType()))\n",
    "# def extract_brands(text):\n",
    "#     brands, _ = extract_brands_and_counts(text)\n",
    "#     return brands\n",
    "\n",
    "# @udf(ArrayType(IntegerType()))\n",
    "# def extract_mention_counts(text):\n",
    "#     _, counts = extract_brands_and_counts(text)\n",
    "#     return counts\n",
    "\n",
    "# df = df.withColumn(\"brand_name\", extract_brands(col(\"clean_text\")))\n",
    "# df = df.withColumn(\"mention_count\", extract_mention_counts(col(\"clean_text\")))\n",
    "\n",
    "# # Filter rows with at least one valid brand mention\n",
    "# df = df.filter(col(\"brand_name\").isNotNull() & (col(\"brand_name\").getItem(0).isNotNull()))\n",
    "# print(f\"Number of rows with brand mentions: {df.count()}\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216c945-8ad4-435a-98a7-abc5191ba2d9",
   "metadata": {},
   "source": [
    "Save Spark Dataframe with brand mentions to Parquet files as a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658bd36-386f-4099-8e9a-7406bffc28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"overwrite\").parquet(\"data/temp/olmo_brand_mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973a4d3-7c0f-4716-90ae-0bdabea777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_mentions_dir = \"data/temp/olmo_brand_mentions\"\n",
    "df = spark.read.parquet(f\"{brand_mentions_dir}/*.parquet\")\n",
    "print(f\"Number of rows with brand mentions: {df.count()}\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed86e6-3d4e-4d22-9dea-54f156e1f56f",
   "metadata": {},
   "source": [
    "### Brand Mentions by Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b32b37-034c-4d60-a214-90afa7cc9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing mentions for each brand\n",
    "for brand in BRANDS:\n",
    "    print(f\"\\n=== Documents mentioning '{brand}' ===\")\n",
    "    brand_df = df.filter(array_contains(col(\"brand_name\"), brand))\n",
    "    brand_df.select(\"clean_text\", \"brand_name\", \"mention_count\").show()\n",
    "    print(f\"Total number of documents in RefinedWeb dataset mentioning '{brand}': {brand_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc79bb-4756-4944-a900-488ed54c2e3f",
   "metadata": {},
   "source": [
    "### Classification of Brand-related Content: *content_type*\n",
    "\n",
    "Content types help tailor sentiment methods, i.e. VADER for user-generated, FinBERT for news)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da944af8-ed57-496b-add8-1116f299352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_content(url, clean_text):\n",
    "    if not isinstance(url, str):\n",
    "        url = \"\"\n",
    "    if not isinstance(clean_text, str):\n",
    "        clean_text = \"\"\n",
    "    \n",
    "    url = url.lower()\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Social media or blogs\n",
    "    user_gen_domains = [\"reddit\", \"twitter\", \"x.com\", \"facebook\", \"linkedin\", \"instagram\", \"tiktok\", \"pinterest\", \"forum\", \"discuss\", \"community\", \"medium\", \"wordpress\", \"blogger\", \"tumblr\", \"substack\", \"blog\"]\n",
    "    if any(domain in url for domain in user_gen_domains):\n",
    "        return \"user_generated\"\n",
    "\n",
    "    # News article: Reputable news sources or news-related keywords\n",
    "    news_domains = [\"bbc\", \"guardian\", \"telegraph\", \"ft.com\", \"reuters\", \"bloomberg\", \"cnn\", \"nytimes\", \"independent\", \"dailymail\", \"sky.com\", \"news\", \"times\"]\n",
    "    news_keywords = [\"breaking news\"]\n",
    "    if any(domain in url for domain in news_domains) or any(keyword in clean_text for keyword in news_keywords):\n",
    "        return \"news_article\"\n",
    "    \n",
    "    # Customer review: Review platforms or review-related keywords\n",
    "    review_keywords = [\"trustpilot\", \"feefo\", \"reviews\", \"yelp\", \"google.com/reviews\"]\n",
    "    if any(domain in url for domain in review_keywords) or any(keyword in clean_text for keyword in review_keywords):\n",
    "        return \"customer_review\"\n",
    "    \n",
    "    # Regulatory document: Official or compliance-related sources or keywords\n",
    "    regulatory_keywords = [\"fca.org.uk\", \"bankofengland\", \"gov.uk\"]\n",
    "    if any(domain in url for domain in regulatory_keywords) or any(keyword in clean_text for keyword in regulatory_keywords):\n",
    "        return \"regulatory_document\"\n",
    "    \n",
    "    # Advertising content: Promotional keywords\n",
    "    advertising_keywords = [\"ads\", \"campaign\", \"promo\", \"sponsor\", \"advert\", \"promotion\", \"ad\"]\n",
    "    if any(term in url for term in advertising_keywords):\n",
    "        return \"advertising_content\"\n",
    "    \n",
    "    # Owned media: Brand or institutional domains or brand mentions\n",
    "    owned_media_domains = [\"gov.uk\", \"ac.uk\", \"co.uk\", \"barclays\", \"lloyds\", \"hsbc\", \"monzo\"]\n",
    "    if any(domain in url for domain in owned_media_domains):\n",
    "        return \"owned_media\"\n",
    "    \n",
    "    # Forum post: Specific forum platforms or discussion keywords\n",
    "    forum_keywords = [\"moneysavingexpert\", \"thestudentroom\", \"forums\"]\n",
    "    if any(domain in url for domain in forum_keywords) or any(keyword in clean_text for keyword in forum_keywords):\n",
    "        return \"forum_post\"\n",
    "    \n",
    "    # FAQ/Knowledge base: Support or informational keywords\n",
    "    faq_keywords = [\"faq\", \"how to\", \"guide\", \"tutorial\"]\n",
    "    if any(keyword in url for keyword in faq_keywords) or any(keyword in clean_text for keyword in faq_keywords):\n",
    "        return \"faq_knowledge_base\"\n",
    "    \n",
    "    # Default: Other\n",
    "    return \"miscellaneous\"\n",
    "\n",
    "content_type_udf = udf(classify_content, StringType())\n",
    "df = df.withColumn(\"content_type\", content_type_udf(col(\"url\"), col(\"clean_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80381bd0-c90e-4d04-a561-f606a6f94e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab61e44-9a38-4c42-9578-61148f0f1cdf",
   "metadata": {},
   "source": [
    "### Summary of Final Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2201b9-0c3c-48de-8534-299eac595447",
   "metadata": {},
   "source": [
    "| Column Name    | Description                                                                 |\n",
    "|----------------|-----------------------------------------------------------------------------|\n",
    "| `text`         | The original textual content of the document record (e.g., the body of an article or code snippet), retained as the primary source text for analysis. |\n",
    "| `clean_text`   | The processed version of the `text` column, where newlines, punctuation, digits, and excessive whitespace are removed, text is lowercased, and emojis are converted to text for consistency in analysis. |\n",
    "| `url`          | The URL of the web page or resource from which the content was sourced, used for content type classification and brand context. |\n",
    "| `brand_name`   | An array of organization names extracted from `clean_text`, representing brand mentions for targeted sentiment analysis. |\n",
    "| `mention_count`| The number of brand mentions (size of the `brand_name` array) in each row, quantifying the frequency of brand references. |\n",
    "| `content_type` | A categorized label (e.g., `user_generated`, `news_article`, `customer_review`, etc.) assigned based on the `url`, indicating the type of content for further analysis. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac84abe-c6e2-4bdb-bfbd-08dbc313ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10282244-9c6a-429c-b667-0283e329b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2073e99-1894-41c8-8049-df3d9423f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361545ab-175d-4f82-83f4-6b96a4e415ea",
   "metadata": {},
   "source": [
    "# 4. Brand Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc797f4e-2979-456e-a957-9ece9bead94d",
   "metadata": {},
   "source": [
    "In this section, sentiment analysis is performed on UK bank brand mentions using a hybrid approach combining lexicon-based (VADER) and a transformer-based model (FinBERT). The aim is to analyze the emotional tone (positive, neutral, negative) of the brand mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a25b6a-5335-4d1d-ae83-d5068a02d650",
   "metadata": {},
   "source": [
    "## 4.1 Lexicon-Based (VADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9acaf3-f714-4086-b039-fc56dcc4c7e5",
   "metadata": {},
   "source": [
    "The following code performs brand sentiment analysis using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner), a lexicon-based tool specifically designed for detecting sentiment in user-generated texts. VADER is fast and handles slang, emojis, and short texts well, making it ideal for analysing sentiment in data sources such as social media and reviews.\n",
    "\n",
    "VADER calculates 4 sentiment metrics for each text input:\n",
    "- `vader_score` (compound score): A normalized weighted composite score ranging from -1 (negative) to +1 (positive). Derived from the sum of valence scores of individual words, adjusted for modifiers (e.g., \"very good\" amplifies positivity).\n",
    "- `positive_score`, `neutral_score`, `negative_score`: Proportional metrics representing the text's positive, neutral, and negative sentiment (each ranges 0–1). The 3 scores sum to 1.\n",
    "\n",
    "`sentiment_label` is assigned based on the compound `vader_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c366d1b-060d-48ee-930a-5b6252155830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculates VADER sentiment\n",
    "def vader_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"compound\": 0.0, \"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0}\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "# Schema for VADER output\n",
    "vader_schema = StructType([\n",
    "    StructField(\"compound\", FloatType(), nullable=True),\n",
    "    StructField(\"pos\", FloatType(), nullable=True),\n",
    "    StructField(\"neu\", FloatType(), nullable=True),\n",
    "    StructField(\"neg\", FloatType(), nullable=True)\n",
    "])\n",
    "\n",
    "vader_udf = udf(vader_sentiment, vader_schema)\n",
    "df = df.withColumn(\"vader_sentiment\", vader_udf(col(\"clean_text\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237b654-ade7-4a37-ab0f-09b00d4479a2",
   "metadata": {},
   "source": [
    "### Sentiment Scores and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a309eb-4e50-4b1f-ac4d-3d7b0d79fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"vader_score\", col(\"vader_sentiment.compound\"))\n",
    "df = df.withColumn(\"positive_score\", col(\"vader_sentiment.pos\"))\n",
    "df = df.withColumn(\"neutral_score\", col(\"vader_sentiment.neu\"))\n",
    "df = df.withColumn(\"negative_score\", col(\"vader_sentiment.neg\"))\n",
    "\n",
    "# Sentiment Label\n",
    "df = df.withColumn(\"sentiment_label\",\n",
    "    when(col(\"vader_score\") > 0.05, \"Positive\")\n",
    "    .when(col(\"vader_score\") < -0.05, \"Negative\")\n",
    "    .otherwise(\"Neutral\"))\n",
    "\n",
    "df = df.drop(\"vader_sentiment\")\n",
    "\n",
    "print(\"\\nVADER Sentiment Scores and Labels:\")\n",
    "df.select(\n",
    "    \"clean_text\", \"brand_name\", \"mention_count\", \"content_type\", \"vader_score\", \"positive_score\",\n",
    "    \"neutral_score\", \"negative_score\", \"sentiment_label\"\n",
    ").show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781b483-c271-4753-8c2d-574a482ed527",
   "metadata": {},
   "source": [
    "### Overall Sentiment Aggregation: *avg_vader_score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed40fcb-a38a-4370-80cd-438bb42e7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associates sentiment with each brand\n",
    "df_exploded = df.select(\n",
    "    explode(col(\"brand_name\")).alias(\"brand\"),\n",
    "    col(\"vader_score\"),\n",
    "    col(\"content_type\")\n",
    ")\n",
    "\n",
    "# Sentiment by brand and content_type\n",
    "sentiment_summary = df_exploded.groupBy(\"brand\", \"content_type\").agg(\n",
    "    avg(\"vader_score\").alias(\"avg_vader_score\")\n",
    ").orderBy(\"brand\", \"content_type\")\n",
    "\n",
    "# Sentiment label\n",
    "sentiment_summary = sentiment_summary.withColumn(\n",
    "    \"avg_sentiment_label\",\n",
    "    when(col(\"avg_vader_score\") > 0.05, \"Positive\")\n",
    "    .when(col(\"avg_vader_score\") < -0.05, \"Negative\")\n",
    "    .otherwise(\"Neutral\")\n",
    ")\n",
    "\n",
    "print(\"VADER Sentiment Summary by Brand and Content Type:\")\n",
    "sentiment_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f54a9-a9d0-4f70-9ce3-48909f170f43",
   "metadata": {},
   "source": [
    "## 4.2 Transformer-Based (FinBERT)\n",
    "**FinBERT** model is implemented for brand sentiment analysis of UK financial services brands due to its:\n",
    "- **Domain-specialisation**:  Explicitly trained on financial texts (10M+ finance docs), including financial news, analyst reports, earnings call transcripts, SEC/FCA filings, and other regulatory documents. It has good understanding of key financial concepts, such as, financial metrics, market movements, and regulatory language.\n",
    "- **Sentiment granularity**: 3-class (positive/neutral/negative)\n",
    "- **Numerical sensitivity**: Handles earnings and percentages well.\n",
    "\n",
    "FinBERT understands context better than VADER, excelling in more complex texts such as news articles, regulatory documents, and reports. \n",
    "\n",
    "The following outputs are computed:\n",
    "- `finbert_label` – the sentiment class with the highest average probability across all chunks\n",
    "- `finbert_score` – the sentiment polarity score, calculated as Positive - Negative probability.\n",
    "- `finbert_confidence`: How confident FinBERT is about its prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81924c-5576-4da3-b8f2-676fa40d724d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# CPU / GPU checks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cpu\":\n",
    "    print(\"Warning: Flash Attention requires a CUDA-capable GPU. Falling back to standard attention.\")\n",
    "else:\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "# Enables Flash Attention\n",
    "torch.backends.cuda.enable_flash_sdp(True)  \n",
    "\n",
    "# FinBERT tokenizer and model\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", use_fast=True)\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "# Additional optimisation using xformers\n",
    "try:\n",
    "    from xformers.ops import memory_efficient_attention\n",
    "    print(\"Using xformers for memory-efficient attention\")\n",
    "except ImportError:\n",
    "    print(\"xformers not installed. Using PyTorch Flash Attention.\")\n",
    "\n",
    "# FinBERT pipeline \n",
    "finbert_pipeline = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=finbert_model,\n",
    "    tokenizer=finbert_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    torch_dtype=torch.float16,  # Keep for memory efficiency\n",
    "    return_all_scores=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1cdaa-95cc-42e7-b7ae-f84c445722af",
   "metadata": {},
   "source": [
    "The large document texts are then split into context-level chunks. That is, each chunk contains a brand mentions and captures ±2 sentences surrounding each mention. \n",
    "\n",
    "The text is first split into individual sentences, with sentences containing brand mentions being flagged. Chunks are consequently formed around each brand mention with ±2 sentences being appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b6cce-cc01-43ef-837f-3303bfd5adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits text into context-based chunks\n",
    "def prepare_chunks(text, tokenizer, brands=None, max_tokens=510):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "        \n",
    "    # If no brands provided, return empty list to avoid processing\n",
    "    if not brands:\n",
    "        return []\n",
    "    \n",
    "    # Split text into sentences using NLTK\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Normalize brands for case-insensitive matching\n",
    "    brands = [brand.lower() for brand in brands]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Identify sentences containing brand mentions\n",
    "    brand_mention_indices = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        # Check if any brand is mentioned in the sentence (case-insensitive)\n",
    "        if any(brand in sentence.lower() for brand in brands):\n",
    "            brand_mention_indices.append(i)\n",
    "    \n",
    "    if not brand_mention_indices:\n",
    "        return []\n",
    "    \n",
    "    # Create chunks around each brand mention\n",
    "    for idx in brand_mention_indices:\n",
    "        # Define context window: ±2 sentences (up to 5 sentences total)\n",
    "        start_idx = max(0, idx - 2)\n",
    "        end_idx = min(len(sentences), idx + 3)  # idx + 2 + 1 to include the mention sentence\n",
    "        context_sentences = sentences[start_idx:end_idx]\n",
    "        \n",
    "        # Initialize chunk and token count\n",
    "        current_chunk = []\n",
    "        current_token_count = 0\n",
    "        \n",
    "        for sentence in context_sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "                \n",
    "            # Tokenize sentence to count tokens\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            token_count = len(tokens)\n",
    "            \n",
    "            # If a single sentence exceeds max_tokens, truncate it\n",
    "            if token_count > max_tokens:\n",
    "                truncated_tokens = tokens[:max_tokens]\n",
    "                truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "                chunks.append(truncated_sentence)\n",
    "                continue\n",
    "                \n",
    "            # If adding sentence exceeds max_tokens, finalize current chunk\n",
    "            if current_token_count + token_count > max_tokens:\n",
    "                if current_chunk:\n",
    "                    chunk_text = \" \".join(current_chunk)\n",
    "                    if chunk_text.strip():\n",
    "                        chunks.append(chunk_text)\n",
    "                current_chunk = [sentence]\n",
    "                current_token_count = token_count\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_chunk.append(sentence)\n",
    "                current_token_count += token_count\n",
    "        \n",
    "        # Append any remaining chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0deaf6-c496-44b2-93d0-e43e97fd9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes sentiment of text using FinBERT, processing chunks around brand mentions\n",
    "def analyze_finbert(text):\n",
    "    global BRANDS  \n",
    "    try:\n",
    "        chunks = prepare_chunks(text, finbert_tokenizer, brands=BRANDS)\n",
    "        if not chunks or all(not c.strip() for c in chunks):\n",
    "            return \"neutral\", 0.0, 0.0, {\"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
    "\n",
    "        results = finbert_pipeline(chunks)\n",
    "        cumulative_scores = {\"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0}\n",
    "        confidences = []\n",
    "        count = 0\n",
    "\n",
    "        for r in results:\n",
    "            if isinstance(r, list):\n",
    "                for entry in r:\n",
    "                    label = entry[\"label\"].lower()\n",
    "                    score = entry[\"score\"]\n",
    "                    cumulative_scores[label] += score\n",
    "                confidences.append(max(entry[\"score\"] for entry in r))\n",
    "                count += 1\n",
    "\n",
    "        if count == 0:\n",
    "            return \"neutral\", 0.0, 0.0, {\"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
    "\n",
    "        # Normalizes all the scores\n",
    "        avg_scores = {k: v / count for k, v in cumulative_scores.items()}\n",
    "        avg_confidence = sum(confidences) / count\n",
    "\n",
    "        # Polarity score\n",
    "        polarity = avg_scores[\"positive\"] - avg_scores[\"negative\"]\n",
    "\n",
    "        # Final predicted label\n",
    "        if abs(polarity) < 0.15:\n",
    "            final_label = \"neutral\"\n",
    "        else:\n",
    "            final_label = \"positive\" if polarity > 0 else \"negative\"\n",
    "\n",
    "        return (\n",
    "            final_label,\n",
    "            round(polarity, 4),\n",
    "            round(avg_confidence, 4),\n",
    "            {k: round(v, 4) for k, v in avg_scores.items()}\n",
    "        )\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        return analyze_finbert_vader_style(text)\n",
    "    except Exception as e:\n",
    "        print(f\"FinBERT error on text {text[:50]}...: {str(e)}\")\n",
    "        return \"neutral\", 0.0, 0.0, {\"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7dfe27-a1a2-42b5-8049-78561aab2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row index for joining\n",
    "df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Convert to Pandas for transformer processing\n",
    "pandas_df = df.select(\"row_id\", \"text\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d0994-d7cc-448d-8983-803e16458234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs analysis\n",
    "finbert_results = [analyze_finbert(text) for text in pandas_df[\"text\"]]\n",
    "\n",
    "# Sentiment label, score and confidence\n",
    "pandas_df[\"finbert_label\"] = [r[0] for r in finbert_results]\n",
    "pandas_df[\"finbert_score\"] = [r[1] for r in finbert_results]\n",
    "pandas_df[\"finbert_confidence\"] = [r[2] for r in finbert_results]\n",
    "\n",
    "# Individual sentiment scores\n",
    "pandas_df[\"finbert_dist_positive\"] = [r[3][\"positive\"] for r in finbert_results]\n",
    "pandas_df[\"finbert_dist_neutral\"] = [r[3][\"neutral\"] for r in finbert_results]\n",
    "pandas_df[\"finbert_dist_negative\"] = [r[3][\"negative\"] for r in finbert_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03afd9b-6a24-42d2-af00-080bb2422fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved to CSV\n",
    "finbert_csv = \"data/finbert_results.csv\"\n",
    "pandas_df[[\n",
    "    \"row_id\", \"finbert_label\", \"finbert_score\", \"finbert_confidence\",\n",
    "    \"finbert_dist_positive\", \"finbert_dist_neutral\", \"finbert_dist_negative\"\n",
    "]].to_csv(finbert_csv, index=False)\n",
    "\n",
    "# Transforms back to Spark\n",
    "transformer_df = spark.read.csv(finbert_csv, header=True, inferSchema=True)\n",
    "df = df.join(transformer_df, \"row_id\").drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84892ce1-9856-4015-bf24-476bacde2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinBERT Sentiment Scores:\")\n",
    "df.select(\n",
    "     \"clean_text\", \"brand_name\", \"mention_count\", \"content_type\",\n",
    "    \"finbert_label\", \"finbert_score\", \"finbert_confidence\", \"finbert_dist_positive\", \"finbert_dist_neutral\", \"finbert_dist_negative\"\n",
    ").show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c259c5-bf9d-4c2f-b5c5-04698d61057e",
   "metadata": {},
   "source": [
    "## 4.3 Hybrid Approach (Combining VADER and FinBERT labels)\n",
    "The following section combines VADER and FinBERT predictions, weighted by `content_type`. VADER is up-weighted for `user_generated` and `customer_review`, and FinBERT for `news_article` and `regulatory_document`. This outputs `hybrid_sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dceb2fc-0d71-4b83-959d-a6fd581e7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def hybrid_sentiment(vader_score, finbert_score, content_type):\n",
    "    if vader_score is None or finbert_score is None:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "    if content_type == \"user_generated\":\n",
    "        combined_score = (vader_score * 0.6) + (finbert_score * 0.4) # adjust weight if necessary\n",
    "    # Use only VADER when finbert_score is 0\n",
    "    else:\n",
    "        # Use only FinBERT for all other content types\n",
    "        combined_score = finbert_score\n",
    "    \n",
    "    # Sentiment thresholds\n",
    "    if combined_score > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif combined_score < -0.05:\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "df = df.withColumn(\"hybrid_sentiment_label\", hybrid_sentiment(\n",
    "    col(\"vader_score\"),\n",
    "    col(\"finbert_score\"),\n",
    "    col(\"content_type\")\n",
    "))\n",
    "\n",
    "print(\"\\nHybrid Sentiment Labels based on VADER and FinBERT results:\")\n",
    "df.select(\n",
    "    \"clean_text\", \"brand_name\", \"mention_count\", \"content_type\", \"hybrid_sentiment_label\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea05365-b1ef-4efc-b0be-4574d64dcb3a",
   "metadata": {},
   "source": [
    "# 5. Brand-Specific Analysis\n",
    "\n",
    "The objective of this section is to delve into sentiment insights for specific brands (e.g Lloyds, Barclays), exploring how sentiment varies by content type, with visualizations for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac738a5b-99f0-4ba9-a410-2f8c79bd0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_sentiment_df = df.select(\n",
    "    explode(arrays_zip(col(\"brand_name\"), col(\"mention_count\"))).alias(\"exploded\"),\n",
    "    col(\"content_type\"),\n",
    "    col(\"vader_score\"),\n",
    "    col(\"sentiment_label\"),\n",
    "    col(\"finbert_label\"),\n",
    "    col(\"finbert_score\"),\n",
    "    col(\"finbert_confidence\"),\n",
    "    col(\"hybrid_sentiment_label\")\n",
    ").select(\n",
    "    col(\"exploded.brand_name\").alias(\"brand\"),\n",
    "    col(\"exploded.mention_count\").alias(\"mentions\"),\n",
    "    col(\"content_type\"),\n",
    "    col(\"vader_score\"),\n",
    "    col(\"sentiment_label\"),\n",
    "    col(\"finbert_label\"),\n",
    "    col(\"finbert_score\"),\n",
    "    col(\"finbert_confidence\"),\n",
    "    col(\"hybrid_sentiment_label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404ad95-8978-4e85-a74a-bfa872b8a406",
   "metadata": {},
   "source": [
    "### Sentiment by Brand and Content Type\n",
    "\n",
    "This reveals which content types drive positive or negative sentiment which can guide brand reputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d92d31-765b-4bd8-b847-e1d26edae48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_specific_df = brand_sentiment_df.filter(col(\"brand\").isin(BRANDS))\n",
    "\n",
    "# Sentiment summaries for each brand\n",
    "for brand in BRANDS:\n",
    "    brand_df = brand_specific_df.filter(col(\"brand\") == brand)\n",
    "    \n",
    "    brand_summary = brand_df.groupBy(\n",
    "        \"brand\", \"content_type\", \"hybrid_sentiment_label\"\n",
    "    ).agg({\"mentions\": \"sum\"}).withColumnRenamed(\"sum(mentions)\", \"total_mentions\")\n",
    "    \n",
    "    print(f\"\\nBrand Sentiment Summary for {brand.capitalize()}:\")\n",
    "    brand_summary.orderBy(\"content_type\", \"hybrid_sentiment_label\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a34a1c-e8e2-4c2f-8d58-56e991c590f0",
   "metadata": {},
   "source": [
    "# 6. Filtering Positive and Negative Brand Mentions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d48856a-5153-4ae0-a3a3-728477171b37",
   "metadata": {},
   "source": [
    "From here onwards, the analysis will be done on a brand-level. The analysis will be done on 1 brand at a time, with HSBC being the first one. Thus, HSBC mentions are filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71a0e0-c7da-4151-89d4-54849eca3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsbc_df = df.filter(array_contains(col(\"brand_name\"), \"hsbc\"))\n",
    "# hsbc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a65d59-359b-429a-a26c-ec647ab5a77a",
   "metadata": {},
   "source": [
    "Extracting only positive and only negative brand mentions for HSBC and saving them to 2 Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41066dd2-847c-41b3-8f28-810660522984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_hsbc_df.write.mode(\"overwrite\").parquet(\"data/filtered_brand_mentions/hsbc_positive_mentions\")\n",
    "# negative_hsbc_df.write.mode(\"overwrite\").parquet(\"data/filtered_brand_mentions/hsbc_negative_mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0027d4-3e97-4f88-9a8b-e5d04fc7b554",
   "metadata": {},
   "source": [
    "# 7. RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bd6c1-6853-4244-9d74-30f47248dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import Field\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53c90f-dc85-4696-8217-efa3a4cc7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lgging for tracking pipeline progress and errors\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7a2be-0675-42ec-9594-a62f9c7a5718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python --timeout 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d741a-14df-4dc0-acfb-128469636d85",
   "metadata": {},
   "source": [
    "Using RAG, the filtered data will be fed into the **OLMo 2 model**. The OLMo 2 Model class was defined using Hugging Face implementation.\n",
    "\n",
    "OLMo 2 model was initialised with **quantization**. Quantization lowers the memory requirements of loading and using a model by storing the weights in a lower precision while trying to preserve as much accuracy as possible. Weights are traditionally stored in full-precision (fp32) floating point representations, but half-precision (fp16 or bf16) have become increasingly popular data types given the large size of models. The chosen OLMo-2-0425-1B-Instruct-GGUF model is already quantized and is suitable for local deployment.\n",
    "\n",
    "The init function initializes the OLMo 2 model. The generate function includes the following arguments:\n",
    "* prompt: Input prompt or question\n",
    "* context: Optional list of context strings to include (this is where we inject sentiment)\n",
    "* max_new_tokens: Maximum number of new tokens to generate\n",
    "* do_sample: Whether to use sampling for generation\n",
    "* top_k: Number of highest probability tokens to consider\n",
    "* top_p: Cumulative probability cutoff for top-p sampling\n",
    "\n",
    "This outputs a string of generated text responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13781323-06b4-40f6-9a24-87c17c9a9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from llama_cpp import Llama  # for GGUF support\n",
    "\n",
    "class OLMo2Model:\n",
    "    def __init__(self, model_path: str = \"allenai/OLMo-2-0425-1B-Instruct-GGUF\"):\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-0425-1B-Instruct\")\n",
    "        \n",
    "        # Load the pre-quantized GGUF model using llama-cpp-python\n",
    "        self.model = Llama(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=0,  # set to a positive number if using GPU\n",
    "            n_ctx=2048,      # context length - adjust based on model capabilities\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Device handling is managed by llama-cpp-python\n",
    "        self.device = \"cpu\"  # GGUF model defaults to CPU; GPU support depends on llama-cpp-python build\n",
    "\n",
    "    def generate(self, prompt: str, context: list = None, max_new_tokens: int = 100, temperature: float = 0.7, top_k: int = 50, top_p: float = 0.95) -> str:\n",
    "\n",
    "        # Combine context with prompt if provided\n",
    "        if context:\n",
    "            context_text = \" \".join(context)\n",
    "            full_prompt = f\"Context: {context_text}\\nQuestion: {prompt}\"\n",
    "        else:\n",
    "            full_prompt = prompt\n",
    "        \n",
    "        # Generate response using the GGUF model\n",
    "        output = self.model(\n",
    "            full_prompt,\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            stop=[\"<|endoftext|>\"]  # Stop token based on OLMo 2 chat template\n",
    "        )\n",
    "        return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Initializing model\n",
    "olmo2 = OLMo2Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9f09e-a896-40c1-85af-3ab2e5ed9b8e",
   "metadata": {},
   "source": [
    "## 7.1 Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f0863-4d0a-490c-bb6b-cb2e4739107f",
   "metadata": {},
   "source": [
    "A vector store is a database that stores text embeddings (numerical representations of text generated by a model like BERT). These embeddings enable efficient similarity searches to retrieve relevant documents based on semantic meaning rather than exact keyword matches. \n",
    "\n",
    "The FAISS index is implemented here. First, the cleaned text is converted into a vector using Sentence Transformer (all-MiniLM-L6-v2). Secondly, we build the FAISS index for fast search by taking all of the dataset vectors. It will then find the most similar items to a new query via FAISS. Lastly, FAISS returns similar vectors and documents.\n",
    "\n",
    "This function processes data, creates vector embeddings from the cleaned text, and creates a vector store. This includes the following arguments:\n",
    "\n",
    "* df: Spark DataFrame containing text data (e.g., hsbc_df)\n",
    "* sentiment_filter: Optional filter for sentiment (e.g., \"Positive\" or \"Negative\")\n",
    "* index_path: File path to save the FAISS index\n",
    "* metadata_path: File path to save the metadata CSV\n",
    "\n",
    "It returns returns a tuple: (FAISS index, list of text data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c5f18-c7f1-40f6-9d62-39ae1ddacb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model \n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def create_vector_store(df, sentiment_filter=None, index_path=\"data/vector_store/hsbc_index.faiss\", metadata_path=\"data/vector_store/metadata.csv\"):\n",
    "    \n",
    "    # Filter dataframe based on sentiment if specified\n",
    "    if sentiment_filter:\n",
    "        df_filtered = df.filter(col(\"hybrid_sentiment_label\") == sentiment_filter)\n",
    "    else:\n",
    "        df_filtered = df\n",
    "        \n",
    "    # Extracts cleaned text and converts to vector embeddings \n",
    "    texts = [row[\"clean_text\"] for row in df_filtered.select(\"clean_text\").collect()]\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Create directories and save the index and metadata\n",
    "    os.makedirs(\"data/vector_store\", exist_ok=True)\n",
    "    faiss.write_index(index, index_path)\n",
    "    metadata = pd.DataFrame({\"text\": texts, \"sentiment\": [sentiment_filter] * len(texts) if sentiment_filter else [\"Mixed\"] * len(texts)})\n",
    "    metadata.to_csv(metadata_path, index=False)\n",
    "    logging.info(f\"Saved vector store to {index_path} and metadata to {metadata_path}\")\n",
    "    return index, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7914e-c014-491f-8258-89674eff4e2c",
   "metadata": {},
   "source": [
    "## 7.2 RAG Service and Query System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d061f2-e2b6-455d-8eaa-ace1a55e8a7d",
   "metadata": {},
   "source": [
    "RAG combines a **retrieval** step (retrieves relevant documents from the vector store) with a **generation** step (using OLMo 2 to generate answers). This enhances the model's responses by grounding them in specific and retrieved context.\n",
    "\n",
    "A custom **retriever** for brand sentiment data is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506dacc-0e21-4924-805c-389bd516005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrandSentimentRetriever(BaseRetriever):\n",
    "    index: any = Field(..., description=\"FAISS index for vector search\")\n",
    "    metadata_df: pd.DataFrame = Field(..., description=\"DataFrame containing metadata\")\n",
    "    embedding_model: any = Field(..., description=\"SentenceTransformer model\")\n",
    "    top_k: int = Field(default=5, description=\"Number of documents to retrieve\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True # allowing non-serializable and non-standard types (like a FAISS index object) \n",
    "\n",
    "    # Retrieves relevant documents based on query embedding similarity\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> list[Document]: # query - User input string to search for; run_manager - optional LangChain run manager \n",
    "        \n",
    "        # Encodes query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        # Searches the index\n",
    "        distances, indices = self.index.search(query_embedding, self.top_k)\n",
    "\n",
    "        # Returns documents with metadata\n",
    "        documents = []\n",
    "        for idx in indices[0]:\n",
    "            row = self.metadata_df.iloc[idx]\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=row['text'],\n",
    "                    metadata={\n",
    "                        'sentiment': row.get('sentiment', 'N/A'),\n",
    "                        'brand': 'HSBC'\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        return documents # retrieved list of documents with content and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1f5ce-3428-4565-9812-ae2560c3835d",
   "metadata": {},
   "source": [
    "OLMo 2 model is integrated with the retriever to generate text responses. The **generator** is defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0f9ad-d4d8-4504-b3ad-4a768fa4bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRAG:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.olmo2 = OLMo2Model()  # instantiated OLMo 2 model\n",
    "\n",
    "    # Formats retrieved documents into a single string for the prompt\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "    # Generates a response using OLMo 2 with retrieved context\n",
    "    def invoke(self, query):\n",
    "        try:\n",
    "            # Retrieves relevant documents\n",
    "            docs = self.retriever.invoke(query)\n",
    "\n",
    "            # Format prompt\n",
    "            prompt = f\"\"\"<|system|>\n",
    "            You are a sentiment analysis expert. Answer based only on your knowledge and the additional context provided.\n",
    "            Provide a ranking of UK banks from best to worst.</s>\n",
    "            <|user|>\n",
    "            Context: {self.format_docs(docs)}\n",
    "            Question: {query}</s>\n",
    "            <|assistant|>\"\"\"\n",
    "\n",
    "            # Model generates a response\n",
    "            response = self.olmo2.generate(prompt)\n",
    "            \n",
    "            return response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            return \"Error generating response\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900ef3-1512-4a27-9b02-9943540eaba6",
   "metadata": {},
   "source": [
    "## 7.4 Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1d53a-d798-4ed9-8420-d690c5b46541",
   "metadata": {},
   "source": [
    "The following main pipeline executes end-to-end sentiment analysis pipeline with RAG experiments and returns positive_rag, negative_rag, and control_rag instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed9554-3da1-4230-855a-ebe20c51a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_pipeline():\n",
    "    try:\n",
    "        logging.info(\"Starting pipeline with RAG\")\n",
    "        \n",
    "        # 1. Loads HSBC data\n",
    "        logging.info(\"Loading HSBC data...\")\n",
    "        hsbc_df = spark.read.parquet(\"data/filtered_brand_mentions/hsbc_*_mentions\")\n",
    "\n",
    "        # 2. Sets up vector stores for positive, negative, and mixed (control) cases\n",
    "        logging.info(\"Creating vector stores...\")\n",
    "        positive_index, positive_texts = create_vector_store(hsbc_df, \"Positive\")\n",
    "        negative_index, negative_texts = create_vector_store(hsbc_df, \"Negative\")\n",
    "        # control_index, control_texts = create_vector_store(hsbc_df)  \n",
    "\n",
    "        # Creating metadata df for each case\n",
    "        positive_metadata = pd.DataFrame({\"text\": positive_texts, \"sentiment\": [\"Positive\"] * len(positive_texts)})\n",
    "        negative_metadata = pd.DataFrame({\"text\": negative_texts, \"sentiment\": [\"Negative\"] * len(negative_texts)})\n",
    "        # control_metadata = pd.DataFrame({\"text\": control_texts, \"sentiment\": [\"Mixed\"] * len(control_texts)})\n",
    "\n",
    "        # 3. Sets up RAG for each experimental case\n",
    "        logging.info(\"Initializing RAG for experiments...\")\n",
    "        positive_retriever = BrandSentimentRetriever(\n",
    "            index=positive_index, metadata_df=positive_metadata, embedding_model=embedding_model, top_k=5\n",
    "        )\n",
    "        negative_retriever = BrandSentimentRetriever(\n",
    "            index=negative_index, metadata_df=negative_metadata, embedding_model=embedding_model, top_k=5\n",
    "        )\n",
    "        # control_retriever = BrandSentimentRetriever(\n",
    "        #     index=control_index, metadata_df=control_metadata, embedding_model=embedding_model, top_k=5\n",
    "        # )\n",
    "        \n",
    "        positive_rag = SentimentRAG(positive_retriever)\n",
    "        negative_rag = SentimentRAG(negative_retriever)\n",
    "        # control_rag = SentimentRAG(control_retriever)  \n",
    "\n",
    "        # 4. Conducts experiments with the specified prompt\n",
    "        query = \"What is the best bank in the UK? Provide a ranking from best to worst\"\n",
    "        logging.info(f\"Testing with query: {query}\")\n",
    "        \n",
    "        # Control case: No RAG context injected, direct generation\n",
    "        print(\"\\nControl Case (No Context):\")\n",
    "        control_response = olmo2.generate(query)  \n",
    "        print(control_response)\n",
    "\n",
    "        # Positive case: RAG with positive HSBC mentions\n",
    "        print(\"\\nPositive Case (With Positive HSBC Context):\")\n",
    "        positive_response = positive_rag.invoke(query)\n",
    "        print(positive_response)\n",
    "\n",
    "        # Negative case: RAG with negative HSBC mentions\n",
    "        print(\"\\nNegative Case (With Negative HSBC Context):\")\n",
    "        negative_response = negative_rag.invoke(query)\n",
    "        print(negative_response)\n",
    "\n",
    "        # 5. Loop for further queries\n",
    "        while True:\n",
    "            user_query = input(\"\\nEnter a new query (or 'quit'): \")\n",
    "            if user_query.lower() == 'quit':\n",
    "                break\n",
    "            print(\"\\nControl Case (No Context, Direct Generation):\", olmo2.generate(user_query))\n",
    "            print(\"Positive Case (With Positive HSBC Context):\", positive_rag.invoke(user_query))\n",
    "            print(\"Negative Case (With Negative HSBC Context):\", negative_rag.invoke(user_query))\n",
    "        return positive_rag, negative_rag, control_rag\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rags = run_sentiment_pipeline()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2ddb0-d512-4c1c-99b9-c3a93a785f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042877c-015d-4c81-9b0f-6c15be08b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
